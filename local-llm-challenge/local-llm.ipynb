{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting Prompt Engineering\n",
    "\n",
    "In this notebook we will explore prompt engineering as a way to improve accuracy of Large Language Models (LLMs). \n",
    "\n",
    "This notebook uses an active use case for an auto manufacturer labeling Google Review data of Dealerships as pertaining to either \"Sales\", \"Service\", \"Both\", or \"Overall\"\n",
    "\n",
    "First you will experiment by loading an LLM within the notebook environment and receiving back labels from the LLM based on the system prompt provided to the model for creating labels.\n",
    "\n",
    "This notebook will load the text content of the review and return an LLM-generated label for the content, then compare that label against the control label to generate an accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Environment \n",
    "\n",
    "[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/build-on-aws/generative-ai-prompt-engineering/blob/main/prompt-engineering-chatbot/prompt-engineering-chatbot.ipynb)\n",
    "\n",
    "\n",
    "This notebook has been designed, written and tested to run on machines with a minimum of 16GB of RAM (32GB preferred). However, if you don't have access to one sign up for a free account on [Amazon SageMaker Studio Lab](https://studiolab.sagemaker.aws/).  Studio Lab is a free machine learning (ML) development environment that provides compute and storage (up to 15GB) at no cost with NO credit card required.\n",
    "\n",
    "You can sign up for Amazon SageMaker Studio Lab here: [https://studiolab.sagemaker.aws/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Jump in!\n",
    "\n",
    "### Libraries\n",
    "First, if needed, install `llama-cpp-python` - a library based on `llama-cpp`, a great open source set of libraries for working and experimenting with the underlying technology of generative AI.  \n",
    "\n",
    "`llama-cpp-python` is extra useful because it allows you to run LLMs in system memory instead of the usual VRAM found in GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra index url didn't want to work with a requirements.txt\n",
    "%pip install pandas\n",
    "%pip install openpyxl\n",
    "%pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama3 8b Model\n",
    "\n",
    "The following cells will load the pretrained model. This model has been quantized down to 5 bits of precision in order to fit on standard computer RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../Meta-Llama-3-8B-Instruct.Q5_K_M.gguf\"\n",
    "DATA_FILE_PATH = \"llm_testing_data.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    chat_format=\"llama-3\",\n",
    "    n_gpu_layers=200, #leave this off unless you have gpu to run against\n",
    "    verbose=False,\n",
    "    n_ctx=8000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines the methods we will use to query and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(\n",
    "        system_message,\n",
    "        review_content\n",
    "):\n",
    "    review_message = \"Review: \" + review_content + \" Classification:\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": review_message},\n",
    "    ]\n",
    "\n",
    "    results = LLM.create_chat_completion(\n",
    "        messages\n",
    "    )\n",
    "\n",
    "    answer = results['choices'][0]['message']['content']\n",
    "\n",
    "    return str(answer).strip()\n",
    "\n",
    "\n",
    "def run_model(system_message, limit=None):\n",
    "    df = pd.read_excel(DATA_FILE_PATH, sheet_name=\"Data\")\n",
    "    successes = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if limit and index >= limit:\n",
    "            break\n",
    "        response = query_model(\n",
    "            system_message,\n",
    "            review_content=row['Review Text']\n",
    "        )\n",
    "        response = \"Service\" if response == \"Repairs\" else response\n",
    "        response = \"Overall\" if response == \"Unsure\" else response\n",
    "        if response == row['Control Label']:\n",
    "            successes += 1\n",
    "        else:\n",
    "            print(\"MISMATCH\")\n",
    "            print(\"Label: \" + row['Control Label'])\n",
    "            print(row['Review Text'])\n",
    "            print(\"LLM: \" + response)\n",
    "\n",
    "        print('Row: ' + str(index + 1) + ' | ' + str((successes / (index + 1)) * 100) + \"%\")\n",
    "    \n",
    "    count = limit if limit else df['Control Label'].count()\n",
    "    print()\n",
    "    print(\"Final Results: \" + str((successes / count) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "This final cell is where the magic happens. Within it we see our prompt to the LLM instructing it to label reviews and the conditions which it should use to label them. \n",
    "\n",
    "Adjust the system message here and run the cell to see if you can get accuracy up or down. \n",
    "\n",
    "Can you break 90% accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an AI assistant tasked with categorizing reviews into one of the following topics: \"Repairs\", \"Sales\", \"Both\", or \"Unsure\".\n",
    "Reviews should default to \"Unsure\".\n",
    "\"Repairs\" refers to vehicle repairs, warranty claims, parts, appointments, updates, maintenance, and labor. Reviews listing a \"service advisor\", \"service department\", \"service center\", \"service technician\", \"warranty\", \"recall\", \"technician\", or any plural form of these words should fall under \"Repairs\" where a sale is also not mentioned. Reviews listing a pre-existing vehicle should fall under \"Repairs\".\n",
    "\"Sales\" refers to vehicle sales, purchasing, test drives, inventory, buyers, shopping, payments, new cars, inventory, financing, and leasing.\n",
    "\"Unsure\" applies to reviews that are generic and do not explicitly mention a sale or repair.\n",
    "Reviews should only qualify for \"Sales\" or \"Repairs\" if a condition is met and only qualify for \"Both\" if a condition from both categories is met.\n",
    "Please respond with just one word.\n",
    "\"\"\"\n",
    "\n",
    "# adjust the \"limit\" parameter to only run through a designated amount of rows. \n",
    "# eg. limit of 10 will only run 10 rows of data\n",
    "# use limit=None to run against all 1000 rows\n",
    "run_model(system_message, limit=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
