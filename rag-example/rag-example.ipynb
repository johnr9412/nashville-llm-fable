{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "Prompt + Data = Big Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Environment \n",
    "\n",
    "[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/build-on-aws/generative-ai-prompt-engineering/blob/main/prompt-engineering-chatbot/prompt-engineering-chatbot.ipynb)\n",
    "\n",
    "\n",
    "This notebook has been designed, written and tested to run on machines with a minimum of 16GB of RAM (32GB preferred). However, if you don't have access to one sign up for a free account on [Amazon SageMaker Studio Lab](https://studiolab.sagemaker.aws/).  Studio Lab is a free machine learning (ML) development environment that provides compute and storage (up to 15GB) at no cost with NO credit card required.\n",
    "\n",
    "You can sign up for Amazon SageMaker Studio Lab here: [https://studiolab.sagemaker.aws/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Example\n",
    "\n",
    "### Boring Stuff\n",
    "This is just code needed to set everything up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
    "%pip install langchain\n",
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%git lfs install\n",
    "%git clone https://huggingface.co/johnr9412/Nashville-Meta-Llama-3-8B-Instruct-GGUF ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../Nashville-Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "LLM = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    chat_format=\"llama-3\",\n",
    "    n_gpu_layers=200, #leave this off unless you have gpu to run against\n",
    "    verbose=False,\n",
    "    n_ctx=8000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "context_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Given the context below, answer the question that follows. If you do not know the answer and the context does not contain the information to answer the question say you don't know and why.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def query_model(user_prompt, additional_context=\"\"):\n",
    "    messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an AI assistant which gives helpful, detailed, and polite answers to the user's questions.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": context_template.format(question=user_prompt, context=additional_context)\n",
    "          }\n",
    "      ]\n",
    "\n",
    "    results = LLM.create_chat_completion(\n",
    "        messages\n",
    "    )\n",
    "    \n",
    "    answer = results['choices'][0]['message']['content']\n",
    "    return f\"Question: {user_prompt}\\nAnswer: {str(answer).strip()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Model\n",
    "First things first: let's ask the model something it won't know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the 2024 Super Bowl?\"\n",
    "\n",
    "response = query_model(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that was lame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model + Data\n",
    "Let's give our model some more data to make it more useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the 2024 Super Bowl?\"\n",
    "additional_context = \"The Kansas City Chiefs won the 2024 Super Bowl 25 to 22 over the San Fransisco 49ers.\"\n",
    "\n",
    "response = query_model(question, additional_context=additional_context)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Example\n",
    "\n",
    "### Store Data\n",
    "Let's take a document wtih some data detailing who won recent Super Bowls.\n",
    "\n",
    "Below is some boilerplate to store our data into a local Vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain is a framework and toolkit for interacting with LLMs programmatically\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders.text import TextLoader\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "text_splitter = CharacterTextSplitter (chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "# Load the document using a LangChain text loader\n",
    "texts = []\n",
    "data_dir = \"./docs/\"\n",
    "for file_path in listdir(data_dir):\n",
    "    file_path = join(data_dir, file_path)\n",
    "    if isfile(file_path):\n",
    "        loader = TextLoader(file_path)\n",
    "\n",
    "        # Split the document into chunks\n",
    "        for doc in text_splitter.split_documents(loader.load()):\n",
    "            texts.append(doc.page_content)\n",
    "\n",
    "# Use the sentence transformer package with the all-MiniLM-L6-v2 embedding model\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the text embeddings in SQLiteVSS in a table named state_union\n",
    "db = Chroma.from_texts(\n",
    "    texts = texts,\n",
    "    embedding = embedding_function\n",
    ")\n",
    "\n",
    "# First, we will do a simple retrieval using similarity search\n",
    "# Query\n",
    "question = \"Who won the 2024 Super Bowl?\"\n",
    "data = db.similarity_search(question, k=1)\n",
    "\n",
    "# print results\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THROW IT ALL TOGETHER. \n",
    "### Automate the \"Retrival\" and \"Augmate\" the \"Generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the 2024 Super Bowl?\"\n",
    "\n",
    "data = db.similarity_search(question, k=1)\n",
    "additional_context = data[0].page_content\n",
    "\n",
    "response = query_model(question, additional_context=additional_context)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
