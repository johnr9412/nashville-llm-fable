{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "Prompt + Data = Big Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Environment \n",
    "\n",
    "[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/build-on-aws/generative-ai-prompt-engineering/blob/main/prompt-engineering-chatbot/prompt-engineering-chatbot.ipynb)\n",
    "\n",
    "\n",
    "This notebook has been designed, written and tested to run on machines with a minimum of 16GB of RAM (32GB preferred). However, if you don't have access to one sign up for a free account on [Amazon SageMaker Studio Lab](https://studiolab.sagemaker.aws/).  Studio Lab is a free machine learning (ML) development environment that provides compute and storage (up to 15GB) at no cost with NO credit card required.\n",
    "\n",
    "You can sign up for Amazon SageMaker Studio Lab here: [https://studiolab.sagemaker.aws/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Example\n",
    "\n",
    "### Boring Stuff\n",
    "This is just code needed to set everything up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"/Users/john.robinson/Projects/models/Meta-Llama-3-8B-Instruct\",\n",
    "        device=\"mps\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "context_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an AI assistant which gives helpful, detailed, and polite answers to the user's questions\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Given the context below, answer the question that follows. If you do not know the answer and the context does not contain the information to answer the question say you don't know and why.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def test_model(pipeline, user_prompt, additional_context=\"\"):\n",
    "\n",
    "    prompt=context_template.format(question=user_prompt, context=additional_context)\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8,\n",
    "        eos_token_id=terminators,\n",
    "        max_new_tokens=200,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=pipeline.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    answer = sequences[0]['generated_text']\n",
    "    \n",
    "    return f\"Question: {user_prompt}\\nAnswer: {answer}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Model\n",
    "First things first: let's ask the model something it won't know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the 2024 Super Bowl?\"\n",
    "\n",
    "response = test_model(pipeline, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that was lame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model + Data\n",
    "Let's give our model some more data to make it more useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the 2024 Super Bowl?\"\n",
    "additional_context = \"The Kansas City Chiefs won the 2024 Super Bowl 25 to 22 over the San Fransisco 49ers.\"\n",
    "\n",
    "response = test_model(pipeline, question, additional_context=additional_context)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Example\n",
    "\n",
    "### Store Data\n",
    "Let's take a document wtih some data detailing who won recent Super Bowls.\n",
    "\n",
    "Below is some boilerplate to store our data into a local Vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain is a framework and toolkit for interacting with LLMs programmatically\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import SQLiteVSS\n",
    "from langchain.document_loaders.text import TextLoader\n",
    "\n",
    "# Load the document using a LangChain text loader\n",
    "loader = TextLoader(\"data.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = CharacterTextSplitter (chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "texts = [doc.page_content for doc in docs]\n",
    "\n",
    "# Use the sentence transformer package with the all-MiniLM-L6-v2 embedding model\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the text embeddings in SQLiteVSS in a table named state_union\n",
    "db = SQLiteVSS.from_texts(\n",
    "    texts = texts,\n",
    "    embedding = embedding_function,\n",
    "    table = \"documents\",\n",
    "    db_file = \"/tmp/vss.db\"\n",
    ")\n",
    "\n",
    "# First, we will do a simple retrieval using similarity search\n",
    "# Query\n",
    "question = \"Who won the 2024 Super Bowl?\"\n",
    "data = db.similarity_search(question)\n",
    "\n",
    "# print results\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THROW IT ALL TOGETHER. \n",
    "### Automate the \"Retrival\" and \"Augmate\" the \"Generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the 2024 Super Bowl?\"\n",
    "\n",
    "data = db.similarity_search(question)\n",
    "additional_context = data[0].page_content\n",
    "\n",
    "response = test_model(pipeline, question, additional_context=additional_context)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
